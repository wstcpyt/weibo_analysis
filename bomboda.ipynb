{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pathlist = glob.glob('data/weibo/*/*/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'attitudes_count': 0,\n",
      " u'base62_id': u'D1rzc8Gtz',\n",
      " u'biz_feature': 0,\n",
      " u'bmiddle_pic': u'http://ww4.sinaimg.cn/bmiddle/bb606f3agw1exihauyoabj20k00qo0u4.jpg',\n",
      " u'category': 31,\n",
      " u'comments_count': 0,\n",
      " u'created_at': u'Fri Oct 30 00:39:52 +0800 2015',\n",
      " u'darwin_tags': [],\n",
      " u'favorited': False,\n",
      " u'geo': None,\n",
      " u'id': 3903442982069905,\n",
      " u'idstr': u'3903442982069905',\n",
      " u'in_reply_to_screen_name': u'',\n",
      " u'in_reply_to_status_id': u'',\n",
      " u'in_reply_to_user_id': u'',\n",
      " u'mid': u'3903442982069905',\n",
      " u'mlevel': 0,\n",
      " u'original_pic': u'http://ww4.sinaimg.cn/large/bb606f3agw1exihauyoabj20k00qo0u4.jpg',\n",
      " u'pic_ids': [u'bb606f3agw1exihauyoabj20k00qo0u4',\n",
      "              u'bb606f3agw1exihavc5pgj20k00qo3zy',\n",
      "              u'bb606f3agw1exihav361qj20k00qo0v4',\n",
      "              u'bb606f3agw1exihavhb0aj20qo0k0tbg',\n",
      "              u'bb606f3agw1exihawscayj20v91jltki'],\n",
      " u'relation': None,\n",
      " u'reposts_count': 1,\n",
      " u'rid': u'0_0_0_2789506320176548201',\n",
      " u'source': u'<a href=\"http://weibo.com/\" rel=\"nofollow\">\\u5fae\\u535a weibo.com</a>',\n",
      " u'source_allowclick': 0,\n",
      " u'source_type': 1,\n",
      " u'status': 0,\n",
      " u'text': u'kate spade,\\u8d2d\\u4e8e\\u7f8e\\u56fd\\u4e13\\u67dc\\uff0c\\u4f7f\\u7528\\u6b21\\u6570\\u4e0d\\u591a\\uff0c\\u6b63\\u5e38\\u4f7f\\u7528\\u75d5\\u8ff9\\u3002\\u539f\\u4ef7\\u4e0d\\u52302000\\u8d2d\\u5165\\uff0c\\u73b0500\\u51fa\\u3002\\u53ef\\u5c0f\\u5200\\uff0c\\u8bda\\u610f\\u4e70\\u7684\\u8bdd\\u53ef\\u4ee5\\u79c1\\u4fe1\\u3002@\\u4e8c\\u624b\\u95f2\\u7f6e\\u8f6c\\u6362 @\\u4e8c\\u624b\\u95f2\\u7f6e\\u5e2e\\u8f6c\\u603b\\u7ad9 @amy\\u5988\\u5356\\u95f2\\u7f6e',\n",
      " u'thumbnail_pic': u'http://ww4.sinaimg.cn/thumbnail/bb606f3agw1exihauyoabj20k00qo0u4.jpg',\n",
      " u'truncated': False,\n",
      " u'url_objects': [],\n",
      " u'user': {u'allow_all_act_msg': False,\n",
      "           u'allow_all_comment': True,\n",
      "           u'avatar_hd': u'http://ww3.sinaimg.cn/crop.40.143.358.358.1024/bb606f3ajw8ewtkvleozwj20c80hxq3g.jpg',\n",
      "           u'avatar_large': u'http://tp3.sinaimg.cn/3143659322/180/5739243073/0',\n",
      "           u'badge': {u'anniversary': 0,\n",
      "                      u'bind_taobao': 0,\n",
      "                      u'dailv': 0,\n",
      "                      u'enterprise': 0,\n",
      "                      u'gongyi': 0,\n",
      "                      u'gongyi_level': 0,\n",
      "                      u'hongbao_2014': 0,\n",
      "                      u'shuang11_2015': 0,\n",
      "                      u'suishoupai_2014': 0,\n",
      "                      u'taobao': 0,\n",
      "                      u'travel2013': 0,\n",
      "                      u'uc_domain': 0,\n",
      "                      u'zongyiji': 0},\n",
      "           u'badge_top': u'',\n",
      "           u'bi_followers_count': 0,\n",
      "           u'block_app': 0,\n",
      "           u'block_word': 0,\n",
      "           u'city': u'1',\n",
      "           u'class': 1,\n",
      "           u'created_at': u'Wed Jan 16 19:53:17 +0800 2013',\n",
      "           u'credit_score': 80,\n",
      "           u'description': u'',\n",
      "           u'domain': u'',\n",
      "           u'extend': {u'mbprivilege': u'0000000000000000000000000000000000000000000000000000000000000000',\n",
      "                       u'privacy': {u'mobile': 1}},\n",
      "           u'favourites_count': 0,\n",
      "           u'follow_me': False,\n",
      "           u'followers_count': 4,\n",
      "           u'following': False,\n",
      "           u'friends_count': 32,\n",
      "           u'gender': u'f',\n",
      "           u'geo_enabled': True,\n",
      "           u'has_ability_tag': 0,\n",
      "           u'id': 3143659322,\n",
      "           u'idstr': u'3143659322',\n",
      "           u'lang': u'zh-cn',\n",
      "           u'level': 1,\n",
      "           u'location': u'\\u6c5f\\u82cf \\u5357\\u4eac',\n",
      "           u'mbrank': 0,\n",
      "           u'mbtype': 0,\n",
      "           u'name': u'\\u674e\\u5706\\u5706\\u5706skrillex',\n",
      "           u'online_status': 0,\n",
      "           u'pagefriends_count': 0,\n",
      "           u'profile_image_url': u'http://tp3.sinaimg.cn/3143659322/50/5739243073/0',\n",
      "           u'profile_url': u'u/3143659322',\n",
      "           u'province': u'32',\n",
      "           u'ptype': 0,\n",
      "           u'remark': u'',\n",
      "           u'screen_name': u'\\u674e\\u5706\\u5706\\u5706skrillex',\n",
      "           u'star': 0,\n",
      "           u'statuses_count': 16,\n",
      "           u'type': 1,\n",
      "           u'ulevel': 0,\n",
      "           u'urank': 3,\n",
      "           u'url': u'',\n",
      "           u'user_ability': 0,\n",
      "           u'verified': False,\n",
      "           u'verified_reason': u'',\n",
      "           u'verified_reason_url': u'',\n",
      "           u'verified_source': u'',\n",
      "           u'verified_source_url': u'',\n",
      "           u'verified_trade': u'',\n",
      "           u'verified_type': -1,\n",
      "           u'weihao': u''},\n",
      " u'userType': 0,\n",
      " u'visible': {u'list_id': 0, u'type': 0}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "dflist = []\n",
    "for i in range(len(pathlist)):\n",
    "    with open(pathlist[i]) as data_file:    \n",
    "        data = json.load(data_file)\n",
    "        d = {'text':data['text'],\"userid\":data['user']['id'],\"province\":data['user']['province'],\"city\":data['user']['city'],\"created_at\":data['user']['created_at'],\"gender\":data['user']['gender']}\n",
    "        newdf = pd.DataFrame(data=d, index=['1'])\n",
    "        dflist.append(newdf)\n",
    "pprint(data)\n",
    "df = pd.concat(dflist, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>created_at</th>\n",
       "      <th>gender</th>\n",
       "      <th>province</th>\n",
       "      <th>text</th>\n",
       "      <th>userid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>Mon Sep 08 18:07:34 +0800 2014</td>\n",
       "      <td>f</td>\n",
       "      <td>31</td>\n",
       "      <td>hhhh关注你这么久终于明白了你一直说自己黑……[笑cry][笑cry][笑cry]（po主...</td>\n",
       "      <td>5283971921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Sun Apr 17 21:57:01 +0800 2011</td>\n",
       "      <td>f</td>\n",
       "      <td>400</td>\n",
       "      <td>回复@努力的芒果粒儿:浑身上下的黑有没有</td>\n",
       "      <td>2104528224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>Mon Sep 08 18:07:34 +0800 2014</td>\n",
       "      <td>f</td>\n",
       "      <td>31</td>\n",
       "      <td>回复@Arielle_K:但是瘦！你说过黑就必要要瘦[嘻嘻]你狠瘦啦[心]</td>\n",
       "      <td>5283971921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Mon Jul 28 15:14:47 +0800 2014</td>\n",
       "      <td>f</td>\n",
       "      <td>400</td>\n",
       "      <td>好看诶这个红色的！</td>\n",
       "      <td>5235009603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000</td>\n",
       "      <td>Sat Jul 17 15:28:03 +0800 2010</td>\n",
       "      <td>f</td>\n",
       "      <td>100</td>\n",
       "      <td>据说这个小羊皮质量很好～</td>\n",
       "      <td>1777517177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon Mar 21 12:05:43 +0800 2011</td>\n",
       "      <td>f</td>\n",
       "      <td>31</td>\n",
       "      <td>光看到好身材了</td>\n",
       "      <td>2036999427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Wed Feb 03 11:37:05 +0800 2010</td>\n",
       "      <td>f</td>\n",
       "      <td>32</td>\n",
       "      <td>方便透露价格么</td>\n",
       "      <td>1689152700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>Sun Apr 17 21:57:01 +0800 2011</td>\n",
       "      <td>f</td>\n",
       "      <td>400</td>\n",
       "      <td>回复@谷甜一:正价310镑</td>\n",
       "      <td>2104528224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>Thu Aug 11 16:32:05 +0800 2011</td>\n",
       "      <td>f</td>\n",
       "      <td>33</td>\n",
       "      <td>MK等他打到RMB2000以下买就值了[偷笑]</td>\n",
       "      <td>2302077230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>Sun Apr 17 21:57:01 +0800 2011</td>\n",
       "      <td>f</td>\n",
       "      <td>400</td>\n",
       "      <td>回复@七月的july:嗯，摸起来就感觉很好</td>\n",
       "      <td>2104528224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   city                      created_at gender province  \\\n",
       "0     6  Mon Sep 08 18:07:34 +0800 2014      f       31   \n",
       "1     2  Sun Apr 17 21:57:01 +0800 2011      f      400   \n",
       "2     6  Mon Sep 08 18:07:34 +0800 2014      f       31   \n",
       "3     2  Mon Jul 28 15:14:47 +0800 2014      f      400   \n",
       "4  1000  Sat Jul 17 15:28:03 +0800 2010      f      100   \n",
       "5     4  Mon Mar 21 12:05:43 +0800 2011      f       31   \n",
       "6     1  Wed Feb 03 11:37:05 +0800 2010      f       32   \n",
       "7     2  Sun Apr 17 21:57:01 +0800 2011      f      400   \n",
       "8     1  Thu Aug 11 16:32:05 +0800 2011      f       33   \n",
       "9     2  Sun Apr 17 21:57:01 +0800 2011      f      400   \n",
       "\n",
       "                                                text      userid  \n",
       "0  hhhh关注你这么久终于明白了你一直说自己黑……[笑cry][笑cry][笑cry]（po主...  5283971921  \n",
       "1                               回复@努力的芒果粒儿:浑身上下的黑有没有  2104528224  \n",
       "2              回复@Arielle_K:但是瘦！你说过黑就必要要瘦[嘻嘻]你狠瘦啦[心]  5283971921  \n",
       "3                                          好看诶这个红色的！  5235009603  \n",
       "4                                       据说这个小羊皮质量很好～  1777517177  \n",
       "5                                            光看到好身材了  2036999427  \n",
       "6                                            方便透露价格么  1689152700  \n",
       "7                                      回复@谷甜一:正价310镑  2104528224  \n",
       "8                            MK等他打到RMB2000以下买就值了[偷笑]  2302077230  \n",
       "9                              回复@七月的july:嗯，摸起来就感觉很好  2104528224  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Word Count 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a: Count the number of posts mentioning each of the included brand names as well as the users who mentioned them. Hint: Pay attention to the variation of names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# we can improve it by using fuzzy match in the future\n",
    "def word_in_text(wordlist,text):\n",
    "    wordlist = map(lambda x: x.lower(),wordlist)\n",
    "    text = text.lower().replace(\" \",\"\")\n",
    "    match = [re.search(word, text) for word in wordlist]\n",
    "    for item in match:\n",
    "        if item:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### number of posts mentioning Kate Spade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5611"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "katespadeseries = df['text'].apply(lambda text: word_in_text([u\"katespade\",u\"凯特·丝蓓\",\"凯特丝蓓\"],text))\n",
    "katespadeseries.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### number of users who mentioned Kate Spade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2388"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[katespadeseries].userid.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "number of posts mentioning Michael Kors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4610"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "michaelkorsseries = df['text'].apply(lambda text: word_in_text([u\"michaelkors\",u\"邁可·寇斯\",\"迈克高仕\"],text))\n",
    "michaelkorsseries.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "number of users who mentioned Michael Kors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1761"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[michaelkorsseries].userid.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   b: List the top 10 users and locations (as province level in China and nation level worldwide) for total posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[\"postcount\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top 10 users for total posts (index:userid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postcount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userid</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3217535201</th>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2187298830</th>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3956831443</th>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5689266274</th>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5671523257</th>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3737544502</th>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161106650</th>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3481576475</th>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5319321035</th>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1811061014</th>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            postcount\n",
       "userid               \n",
       "3217535201        591\n",
       "2187298830        271\n",
       "3956831443        188\n",
       "5689266274        155\n",
       "5671523257        146\n",
       "3737544502        143\n",
       "1161106650        112\n",
       "3481576475         98\n",
       "5319321035         88\n",
       "1811061014         79"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"userid\").sum().sort_values(by=\"postcount\",ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top 10 locations as province level in china\n",
    "\n",
    "check [province city code](http://open.weibo.com/wiki/%E7%9C%81%E4%BB%BD%E5%9F%8E%E5%B8%82%E7%BC%96%E7%A0%81%E8%A1%A8) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['province'] = map(lambda x: int(x),df.province)\n",
    "df['city'] = map(lambda x: int(x),df.city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "province\n",
       "44    2180\n",
       "31    1761\n",
       "11    1681\n",
       "33    1254\n",
       "32    1109\n",
       "51     936\n",
       "42     586\n",
       "81     573\n",
       "35     462\n",
       "37     447\n",
       "Name: postcount, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.province<99].groupby(\"province\").sum().sort_values(by=\"postcount\",ascending=False)[\"postcount\"][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top 10 locations worldwide for total posts\n",
    "\n",
    "1 => \"美国\", 2 => \"英国\", 3 => \"法国\", 4 => \"俄罗斯\", 5 => \"加拿大\", \n",
    "6 => \"巴西\", 7 => \"澳大利亚\", 8 => \"印尼\", 9 => \"泰国\", 10 => \"马来西亚\",\n",
    "11 => \"新加坡\", 12 => \"菲律宾\", 13 => \"越南\", 14 => \"印度\", 15 => \"日本\", 16 => \"其他\"\n",
    "\n",
    "check [province city code](http://open.weibo.com/wiki/%E7%9C%81%E4%BB%BD%E5%9F%8E%E5%B8%82%E7%BC%96%E7%A0%81%E8%A1%A8) for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "china has the most posts:14901"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14901"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chinapostnumber = len(df[df.province<99])\n",
    "chinapostnumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yutongpang/.virtualenvs/bombodaven27/lib/python2.7/site-packages/pandas/core/frame.py:1997: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"DataFrame index.\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "city\n",
       "1     4439\n",
       "16     402\n",
       "5      279\n",
       "2      205\n",
       "7      119\n",
       "15     113\n",
       "3       87\n",
       "11      43\n",
       "4       28\n",
       "10      14\n",
       "Name: postcount, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.province==400][df.city<=16].groupby(\"city\").sum().sort_values(by=\"postcount\",ascending=False)[\"postcount\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Word count II: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a: Find the date that has the highest number of posts mentioning each of the brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['created_at'] = pd.to_datetime(df.created_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_date = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "df_date['date'] = map(lambda x: datetime.datetime(x.year,x.month,x.day,0,0), df_date.created_at)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Michael Kors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date\n",
       "2015-08-29    146\n",
       "2015-08-08    145\n",
       "2013-08-22    123\n",
       "2013-12-26    102\n",
       "2011-09-22     70\n",
       "2011-06-28     62\n",
       "2011-06-15     52\n",
       "2014-11-26     43\n",
       "2013-09-01     41\n",
       "2013-03-09     40\n",
       "Name: postcount, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "michaelkorsseries = df_date['text'].apply(lambda text: word_in_text([u\"michaelkors\",u\"邁可·寇斯\",\"迈克高仕\"],text))\n",
    "df_date[michaelkorsseries].groupby(\"date\").sum().sort_values(by=\"postcount\",ascending=False)['postcount'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Kate Spade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date\n",
       "2013-12-26    54\n",
       "2013-05-20    46\n",
       "2013-04-26    39\n",
       "2011-07-22    39\n",
       "2012-07-24    38\n",
       "2011-05-30    35\n",
       "2014-10-14    34\n",
       "2015-01-18    33\n",
       "2011-03-30    31\n",
       "2011-05-09    29\n",
       "Name: postcount, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "katespadeseries = df_date['text'].apply(lambda text: word_in_text([u\"katespade\",u\"凯特·丝蓓\",\"凯特丝蓓\"],text))\n",
    "df_date[katespadeseries].groupby(\"date\").sum().sort_values(by=\"postcount\",ascending=False)['postcount'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b: Find the peak hour with the most posts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_hour = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_hour['hour'] = map(lambda x: x.hour, df_hour.created_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hour\n",
       "5     1852\n",
       "10    1647\n",
       "6     1553\n",
       "12    1535\n",
       "14    1534\n",
       "3     1499\n",
       "13    1451\n",
       "11    1419\n",
       "2     1341\n",
       "8     1323\n",
       "Name: postcount, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hour.groupby(\"hour\").sum().sort_values(by=\"postcount\",ascending=False)['postcount'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Word count III: \n",
    " Tokenize the comments and retrieve the top 10 mentioned Chinese terms associated with each brand from all texts. You may use 3rd party libraries such as Jieba to complete this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Kate Spade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "katespadeseries = df['text'].apply(lambda text: word_in_text([u\"katespade\",u\"凯特·丝蓓\",\"凯特丝蓓\"],text))\n",
    "kate_df = df[katespadeseries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /Users/yutongpang/.virtualenvs/bombodaven27/lib/python2.7/site-packages/jieba/dict.txt ...\n",
      "DEBUG:jieba:Building prefix dict from /Users/yutongpang/.virtualenvs/bombodaven27/lib/python2.7/site-packages/jieba/dict.txt ...\n",
      "Loading model from cache /var/folders/v6/jvr0q_y16qz5zn12sqs76g9h0000gn/T/jieba.cache\n",
      "DEBUG:jieba:Loading model from cache /var/folders/v6/jvr0q_y16qz5zn12sqs76g9h0000gn/T/jieba.cache\n",
      "Loading model cost 0.336 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.336 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "sentencelist = map(lambda text: list(jieba.cut(text,cut_all=True)),kate_df.text)\n",
    "reducedwordlist = reduce(lambda x,y:x+y,sentencelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'', 61699),\n",
       " (u'\\u7684', 3460),\n",
       " (u'spade', 3173),\n",
       " (u'Kate', 2448),\n",
       " (u'\\u5305', 2061),\n",
       " (u'kate', 2034),\n",
       " (u'Spade', 1598),\n",
       " (u'#', 1221),\n",
       " (u't', 1184),\n",
       " (u'cn', 1179),\n",
       " (u'http', 1159),\n",
       " (u'\\u4e86', 1159),\n",
       " (u'\\u7f8e\\u56fd', 1155),\n",
       " (u'\\u6298', 1095),\n",
       " (u'\\u76f4', 916),\n",
       " (u'\\u6298\\u6263', 848),\n",
       " (u'\\u5230\\u624b', 769),\n",
       " (u'\\u90ae', 748),\n",
       " (u'\\u662f', 742),\n",
       " (u'\\u6709', 709),\n",
       " (u'\\u90fd', 651),\n",
       " (u'\\u94b1\\u5305', 631),\n",
       " (u'\\u6b3e', 616),\n",
       " (u'\\u6211', 607),\n",
       " (u'\\u4ee3\\u8d2d', 602),\n",
       " (u'\\u8272', 541),\n",
       " (u'#kate', 533),\n",
       " (u'\\u56fe', 529),\n",
       " (u'\\u54e6', 506),\n",
       " (u'#Kate', 500)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(Counter(reducedwordlist).items(),key=lambda wordtuple: wordtuple[1],reverse=True)[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The top 10 mentioned chinese terms which is meaningful is:\n",
    "\n",
    "包,美国,折,直(邮),折扣,到手,邮,钱包,款,我,代购"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Michael Kors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "michaelkorsseries = df['text'].apply(lambda text: word_in_text([u\"michaelkors\",u\"邁可·寇斯\",\"迈克高仕\"],text))\n",
    "michael_df = df[michaelkorsseries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentencelist = map(lambda text: list(jieba.cut(text,cut_all=True)),michael_df.text)\n",
    "reducedwordlist = reduce(lambda x,y:x+y,sentencelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'', 59636),\n",
       " (u'Michael', 2893),\n",
       " (u'\\u7684', 2584),\n",
       " (u'Kors', 2086),\n",
       " (u'\\u5305', 2020),\n",
       " (u'kors', 1394),\n",
       " (u'\\u7f8e\\u56fd', 1135),\n",
       " (u'#', 979),\n",
       " (u't', 866),\n",
       " (u'cn', 864),\n",
       " (u'http', 845),\n",
       " (u'\\u4e2d\\u53f7', 796),\n",
       " (u'\\u76f4', 739),\n",
       " (u'\\u4e86', 729),\n",
       " (u'\\u662f', 720),\n",
       " (u'#Michael', 691),\n",
       " (u'\\u8272', 681),\n",
       " (u'\\u4ee3\\u8d2d', 660),\n",
       " (u'\\u90ae', 641),\n",
       " (u'\\u6b3e', 590),\n",
       " (u'\\u6709', 583),\n",
       " (u'\\u552e\\u4ef7', 569),\n",
       " (u'\\u4e13\\u67dc', 565),\n",
       " (u'MK', 562),\n",
       " (u'\\u6b63\\u54c1', 560),\n",
       " (u'\\u5c3a\\u5bf8', 539),\n",
       " (u'\\u989c\\u8272', 538),\n",
       " (u'\\u6298\\u6263', 529),\n",
       " (u'MICHAEL', 496),\n",
       " (u'\\u90fd', 491)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(Counter(reducedwordlist).items(),key=lambda wordtuple: wordtuple[1],reverse=True)[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The top 10 mentioned chinese terms which is meaningful is:\n",
    "\n",
    "包,美国,中号,直(邮),色,代购,邮,款,有,售价"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Sampling\n",
    "Explain possible sampling bias through Weibo, such as gender bias, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 87.4% of posts of post by female user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8743724847529353"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df.gender==\"f\"])/float(len(df[df.gender==\"f\"])+len(df[df.gender==\"m\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gender, age, suburban or city, jobs, can be sources of sampling bias.\n",
    "For example, female user tend to use weibo more than male user, and young people tend to use weibo more than old people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5\n",
    "What are some possible algorithms to identify users who showed interest in Michael Kors over Kate Spade? What are some data points that can be used to illustrate the algorithm's utility? Give a couple examples and discuss pros and cons. (You do not have to code here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be a classifier problem in machine learning:\n",
    "\n",
    "We first need to choose the predictor variables for the problem, including how many posts the user post related to eacd brand, how many reposts related to each brand, how many comments related to each brand, the age of the user, the gender of the user, city and provinces inforamtion...\n",
    "\n",
    "After choosing the predictor variables, we can choose the machine learning algorithm (Decision tree, logistic regression, SVM, neural network) as the model.\n",
    "\n",
    "Then we can divided the data set into traning data set, cross validaiton data set and test data set to train the model using cross validation method to choose the right regularization parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. What algorithms would you use to implement sentiment analysis? List pros & cons of each approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis is a classifier problem in machine learning.\n",
    "\n",
    "Before choosing the machine learning algorithm, we need to select the features (predictor variables), including a dictionary of positive and negative word, emotions sign, emoji...\n",
    "\n",
    "After choosing the features, we can choose the machine learnign model:\n",
    "\n",
    "1. Decision Tree:\n",
    "\n",
    "    pros: interpretability, the decision tree does automatic feature selection, is faster to build and has less  \n",
    "    parameters to tune.\n",
    "    \n",
    "    cons: It ussally has variance (overfitting issues). It can be extremely sensitive to small pertuabations in the       data.\n",
    "2. Support Vector Machine:\n",
    "\n",
    "    pros: It is effective in high dimensional spaces.\n",
    "    \n",
    "    cons: We need to chose kernel carefully, computation performance.\n",
    "3. Neural Network:\n",
    "\n",
    "    pros: simple to implement\n",
    "    \n",
    "    cons: black box, take long time to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. How would you be able to identify brand names in text?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need a dictionary includinng varitions of brand names. (The dictionary should including both chinese and eneglish version of the brand name, and we also need to remove the space between the word, and for the english word, we convert them into lower case.). We also need to take into account the typographical erros and new combinations of the name (fuzzy based word similarity function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = \"michaelkors\"\n",
    "b = \"maahaelkorsdwdasd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match\n"
     ]
    }
   ],
   "source": [
    "r = regex.compile('('+a+'){e<=3}')\n",
    "if r.match(b):\n",
    "    print \"match\"\n",
    "else:\n",
    "    print \"don't match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
